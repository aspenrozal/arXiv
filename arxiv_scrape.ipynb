{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import urllib, urllib.request\r\n",
    "import feedparser \r\n",
    "import time\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------\r\n",
    "# Example from: https://static.arxiv.org/static/arxiv.marxdown/0.1/help/api/examples/python_arXiv_paging_example.txt\r\n",
    "# https://arxiv.org/search/advanced?advanced=1&terms-0-operator=AND&terms-0-term=fairness&terms-0-field=all&terms-1-operator=AND&terms-1-term=variance&terms-1-field=all&terms-2-operator=OR&terms-2-term=algorithmic+bias&terms-2-field=title&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=all_dates&date-year=&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first\r\n",
    "# # Base api query url\r\n",
    "base_url = 'http://export.arxiv.org/api/query?';\r\n",
    "\r\n",
    "# Search parameters\r\n",
    "# Query: \"algorithmic fairness\" OR \"algorithmic bias\" OR \"disparate impact\" OR \"equal opportunity\" OR \"equality of opportunity\" OR equalized odds\" AND \"variance\" OR \"variation\" OR \"variability\" OR \"standard deviation\"\r\n",
    "search_query = '%28all:%22algorithmic+fairness%22+OR+all:%22algorithmic+bias%22+OR+all:%22disparate+impact%22+OR+all:%22equal+opportunity%22+OR+all:%22equality+of+opportunity%22+OR+all:%22equalized+odds%22%29+AND+%28all:%22variance%22+OR+all:%22variability%22+OR+all:%22variation%22+all:%22standard+deviation%22%29'\r\n",
    "\r\n",
    "# published = \r\n",
    "start = 0                       # start at the first result\r\n",
    "total_results = 20               # want x total results\r\n",
    "results_per_iteration = 5       # 5 results at a time\r\n",
    "wait_time = 3                   # number of seconds to wait beetween calls\r\n",
    "sort_type = 'submittedDate'     # submittedDate or relevance or lastUpdatedDate\r\n",
    "sort_order = 'descending'        # Ascending or descending\r\n",
    "\r\n",
    "print ('Searching arXiv for %s' % search_query)\r\n",
    "\r\n",
    "#Establish the list outside the loop\r\n",
    "#List to store metadata entries which will be used to create the bar chart\r\n",
    "metadata = list();\r\n",
    "\r\n",
    "# Original:\r\n",
    "for i in range(start, total_results, results_per_iteration):\r\n",
    "# for i in range(start,results_per_iteration):\r\n",
    "    \r\n",
    "    print (\"Results %i - %i\" % (i,i+results_per_iteration))\r\n",
    "    \r\n",
    "    query = 'search_query=%s&start=%i&max_results=%i&sortBy=%s&sortOrder=%s' % (search_query,\r\n",
    "                                                         i,\r\n",
    "                                                        results_per_iteration,\r\n",
    "                                                        sort_type,\r\n",
    "                                                        sort_order)\r\n",
    "\r\n",
    "    # perform a GET request using the base_url and query\r\n",
    "    response = urllib.request.urlopen(base_url+query).read()\r\n",
    "\r\n",
    "    # parse the response using feedparser\r\n",
    "    feed = feedparser.parse(response)\r\n",
    "\r\n",
    "    # Run through each entry, and print out information\r\n",
    "   \r\n",
    "    for entry in feed.entries:\r\n",
    "        # print ('arxiv-id: %s' % entry.id.split('/abs/')[-1])\r\n",
    "        # print ('Title:  %s' % entry.title)\r\n",
    "        # # feedparser v4.1 only grabs the first author\r\n",
    "        # print ('First Author:  %s' % entry.author)\r\n",
    "        # print ( 'Date: %s ' % entry.date)\r\n",
    "\r\n",
    "        metadataEntry = list()\r\n",
    "\r\n",
    "        # Adding a seperator so that we get a cleaner date without the exact time\r\n",
    "        sep = \"-\"\r\n",
    "\r\n",
    "        metadataEntry.append( entry.date.split( sep )[0] )\r\n",
    "        metadataEntry.append( entry.category )\r\n",
    "\r\n",
    "        metadata.append( metadataEntry )\r\n",
    "\r\n",
    "    # Remember to play nice and sleep a bit before you call\r\n",
    "    # the api again!\r\n",
    "    print ('Sleeping for %i seconds' % wait_time )\r\n",
    "    time.sleep(wait_time)\r\n",
    "\r\n",
    "# print( \"LENGTH\", len(metadata) )\r\n",
    "# print( metadata )\r\n",
    "\r\n",
    "################################################################\r\n",
    "# Visualization \r\n",
    "# data organization: https://www.geeksforgeeks.org/pandas-groupby-count-occurrences-in-column/\r\n",
    "################################################################\r\n",
    "df = pd.DataFrame( metadata, columns = [ 'date', 'category' ] )\r\n",
    "# print( df )\r\n",
    "\r\n",
    "# List of unique values to use for naming stacks\r\n",
    "# print( np.unique(df.category) )\r\n",
    "\r\n",
    "# Get the counts of the categorical data to match with year\r\n",
    "df2 = df.groupby( [ 'date', 'category' ] ).size()\r\n",
    "print( df2 )\r\n",
    "\r\n",
    "# Plot\r\n",
    "df2.plot( kind = \"bar\", stacked = True )"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'feedparser'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6562d97a59fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'feedparser'"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "8154570b195a79e6de28f40c505e14ab94aadf60b74064acb7449cca40c68c90"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}