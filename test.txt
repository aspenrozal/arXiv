<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Afairness%20AND%20variance%26id_list%3D%26start%3D5%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:fairness AND variance&amp;id_list=&amp;start=5&amp;max_results=5</title>
  <id>http://arxiv.org/api/W+EZVprC6xB3GOoDDG2OndrhXpQ</id>
  <updated>2021-07-26T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">96</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/1208.3085v1</id>
    <updated>2012-08-15T10:49:22Z</updated>
    <published>2012-08-15T10:49:22Z</published>
    <title>Variance based Scheduling to Improve the QoS Performance at the Cell
  Edge</title>
    <summary>  Now a days mobile phones are most often used for data communication rather
than voice calls. Due to this change in user behavior, there is a need to
improve the QoS received by the user. One of the ways of improving the QoS is
an efficient scheduling algorithm which incorporates the needs of the users and
variation in channel condition. The parameters used to measure the efficiency
of the scheduling algorithms are the Jain Fairness Index and the overall system
throughput. In this paper we have proposed a variance based scheduling
algorithm which selects the user who has the highest variance of data
transmitted in a given time frame as a parameter for scheduling. This ensures
that eventually, the users transmit almost equal amounts of data regardless of
channel condition. The simulation results shows that the proposed algorithm
achieves high Jain Fairness Index of 0.92 with a lesser drop in the system
throughput 18% as compared to Dynamically altering Proportionally Fair
Algorithm's 20% using the Proportionally Fair Algorithm as reference.
</summary>
    <author>
      <name>Trshant Bhat</name>
    </author>
    <author>
      <name>Sanjay Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, submitted to IEEE ANTS 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.3085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.3085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.02407v2</id>
    <updated>2021-05-31T00:06:53Z</updated>
    <published>2020-11-04T17:00:54Z</published>
    <title>Debiasing classifiers: is reality at variance with expectation?</title>
    <summary>  We present an empirical study of debiasing methods for classifiers, showing
that debiasers often fail in practice to generalize out-of-sample, and can in
fact make fairness worse rather than better. A rigorous evaluation of the
debiasing treatment effect requires extensive cross-validation beyond what is
usually done. We demonstrate that this phenomenon can be explained as a
consequence of bias-variance trade-off, with an increase in variance
necessitated by imposing a fairness constraint. Follow-up experiments validate
the theoretical prediction that the estimation variance depends strongly on the
base rates of the protected class. Considering fairness--performance trade-offs
justifies the counterintuitive notion that partial debiasing can actually yield
better results in practice on out-of-sample data.
</summary>
    <author>
      <name>Ashrya Agrawal</name>
    </author>
    <author>
      <name>Florian Pfisterer</name>
    </author>
    <author>
      <name>Bernd Bischl</name>
    </author>
    <author>
      <name>Francois Buet-Golfouse</name>
    </author>
    <author>
      <name>Srijan Sood</name>
    </author>
    <author>
      <name>Jiahao Chen</name>
    </author>
    <author>
      <name>Sameena Shah</name>
    </author>
    <author>
      <name>Sebastian Vollmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.02407v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.02407v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01, 68Q32, 68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.4; I.2.0; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.07255v2</id>
    <updated>2019-09-10T16:58:05Z</updated>
    <published>2018-11-18T01:54:24Z</published>
    <title>Bayesian Modeling of Intersectional Fairness: The Variance of Bias</title>
    <summary>  Intersectionality is a framework that analyzes how interlocking systems of
power and oppression affect individuals along overlapping dimensions including
race, gender, sexual orientation, class, and disability. Intersectionality
theory therefore implies it is important that fairness in artificial
intelligence systems be protected with regard to multi-dimensional protected
attributes. However, the measurement of fairness becomes statistically
challenging in the multi-dimensional setting due to data sparsity, which
increases rapidly in the number of dimensions, and in the values per dimension.
We present a Bayesian probabilistic modeling approach for the reliable,
data-efficient estimation of fairness with multi-dimensional protected
attributes, which we apply to two existing intersectional fairness metrics.
Experimental results on census data and the COMPAS criminal justice recidivism
dataset demonstrate the utility of our methodology, and show that Bayesian
methods are valuable for the modeling and measurement of fairness in an
intersectional context.
</summary>
    <author>
      <name>James Foulds</name>
    </author>
    <author>
      <name>Rashidul Islam</name>
    </author>
    <author>
      <name>Kamrun Keya</name>
    </author>
    <author>
      <name>Shimei Pan</name>
    </author>
    <link href="http://arxiv.org/abs/1811.07255v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07255v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.04794v1</id>
    <updated>2020-03-10T15:13:05Z</updated>
    <published>2020-03-10T15:13:05Z</published>
    <title>Addressing multiple metrics of group fairness in data-driven decision
  making</title>
    <summary>  The Fairness, Accountability, and Transparency in Machine Learning (FAT-ML)
literature proposes a varied set of group fairness metrics to measure
discrimination against socio-demographic groups that are characterized by a
protected feature, such as gender or race.Such a system can be deemed as either
fair or unfair depending on the choice of the metric. Several metrics have been
proposed, some of them incompatible with each other.We do so empirically, by
observing that several of these metrics cluster together in two or three main
clusters for the same groups and machine learning methods. In addition, we
propose a robust way to visualize multidimensional fairness in two dimensions
through a Principal Component Analysis (PCA) of the group fairness metrics.
Experimental results on multiple datasets show that the PCA decomposition
explains the variance between the metrics with one to three components.
</summary>
    <author>
      <name>Marius Miron</name>
    </author>
    <author>
      <name>Songül Tolan</name>
    </author>
    <author>
      <name>Emilia Gómez</name>
    </author>
    <author>
      <name>Carlos Castillo</name>
    </author>
    <link href="http://arxiv.org/abs/2003.04794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.04794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.09851v1</id>
    <updated>2020-10-19T20:42:18Z</updated>
    <published>2020-10-19T20:42:18Z</published>
    <title>Can I Trust My Fairness Metric? Assessing Fairness with Unlabeled Data
  and Bayesian Inference</title>
    <summary>  We investigate the problem of reliably assessing group fairness when labeled
examples are few but unlabeled examples are plentiful. We propose a general
Bayesian framework that can augment labeled data with unlabeled data to produce
more accurate and lower-variance estimates compared to methods based on labeled
data alone. Our approach estimates calibrated scores for unlabeled examples in
each group using a hierarchical latent variable model conditioned on labeled
examples. This in turn allows for inference of posterior distributions with
associated notions of uncertainty for a variety of group fairness metrics. We
demonstrate that our approach leads to significant and consistent reductions in
estimation error across multiple well-known fairness datasets, sensitive
attributes, and predictive models. The results show the benefits of using both
unlabeled data and Bayesian inference in terms of assessing whether a
prediction model is fair or not.
</summary>
    <author>
      <name>Disi Ji</name>
    </author>
    <author>
      <name>Padhraic Smyth</name>
    </author>
    <author>
      <name>Mark Steyvers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.09851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
